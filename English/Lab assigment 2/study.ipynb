{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.0\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import cma\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymoo.core.problem import Problem\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.factory import get_termination\n",
    "import pymoo\n",
    "print(pymoo.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"xor\": {\n",
    "        \"train\": \"./dat/train_xor.dat\",\n",
    "        \"test\": \"./dat/test_xor.dat\"\n",
    "    },\n",
    "    \"compas\": {\n",
    "        \"train\": \"./dat/train_compas.dat\",\n",
    "        \"test\": \"./dat/test_compas.dat\"\n",
    "    },\n",
    "    \"nomnist\": {\n",
    "        \"train\": \"./dat/train_nomnist.dat\",\n",
    "        \"test\": \"./dat/test_nomnist.dat\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hiperparameters values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores discretos para las capas y las neuronas\n",
    "if True:    \n",
    "    layers_range = [1, 2, 3, 4]\n",
    "    neurons_options = [2, 4, 8, 16, 32, 64, 128]\n",
    "    offline_options = [0, 1]\n",
    "    error_function_options = [0, 1]\n",
    "\n",
    "    # Rango continuo para eta y mu\n",
    "    eta_range = (0.0001, 0.7)\n",
    "    mu_range = (0.0001, 1.0)\n",
    "else:\n",
    "    layers_range = [1]\n",
    "    neurons_options = [2, 4]\n",
    "    offline_options = [0, 1]\n",
    "    error_function_options = [0, 1]\n",
    "\n",
    "    # Rango continuo para eta y mu\n",
    "    eta_range = (0.01, 0.1)\n",
    "    mu_range = (0.01, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test_regex = r\"Train error \\(Mean \\+- SD\\): ([\\d\\.eE+-]+) \\+- ([\\d\\.eE+-]+)\\nTest error \\(Mean \\+- SD\\):\\s+([\\d\\.eE+-]+) \\+- ([\\d\\.eE+-]+)\"\n",
    "#train_test_regex = r\"Train error \\(Mean \\+- SD\\): ([\\d\\.eE+-]+) \\+- ([\\d\\.eE+-]+)\\s*Test\\s+error \\(Mean \\+- SD\\): ([\\d\\.eE+-]+) \\+- ([\\d\\.eE+-]+)\"\n",
    "#train_test_regex = r\"Train\\s+error\\s+\\(Mean\\s+\\+-\\s+SD\\):\\s*([\\d\\.eE+-]+)\\s+\\+-\\s+([\\d\\.eE+-]+)\\s*Test\\s+error\\s+\\(Mean\\s+\\+-\\s+SD\\):\\s*([\\d\\.eE+-]+)\\s+\\+-\\s+([\\d\\.eE+-]+)\"\n",
    "train_test_regex = r\"Train\\s+error\\s+\\(Mean\\s+\\+-\\s+SD\\):\\s*([\\d\\.eE+-]+)\\s+\\+-\\s+([\\d\\.eE+-]+)\\s*Test\\s+error\\s+\\(Mean\\s+\\+-\\s+SD\\):\\s*([\\d\\.eE+-]+)\\s+\\+-\\s+([\\d\\.eE+-]+)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CMA-ES ALGORTIHM OPTIMIZATION - LOOKING FOR THE BEST SET OF PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_program(train_file, test_file, eta, mu, layers, neurons, offline, error_function):\n",
    "    # Construir el comando con las nuevas flags\n",
    "    command = [\n",
    "        \"./bin/la2\", \"-t\", train_file, \"-T\", test_file,\n",
    "        \"-e\", str(eta), \"-m\", str(mu), \"-l\", str(layers), \"-h\", str(neurons), \"-i\", \"2000\",\n",
    "        \"-f\", str(error_function)\n",
    "    ]\n",
    "    # Añadir el modo offline si corresponde\n",
    "    if offline == 1:\n",
    "        command.append(\"-o\")\n",
    "\n",
    "    # Ejecutar el programa\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "    # Buscar los valores de error\n",
    "    match = re.search(train_test_regex, result.stdout, re.DOTALL)\n",
    "    if match:\n",
    "        train_mean, train_std, test_mean, test_std = match.groups()\n",
    "        return {\n",
    "            \"train_mean\": float(train_mean),\n",
    "            \"train_std\": float(train_std),\n",
    "            \"test_mean\": float(test_mean),\n",
    "            \"test_std\": float(test_std)\n",
    "        }\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fitness(params, train_file, test_file):\n",
    "    # Limitar eta y mu dentro de sus rangos permitidos\n",
    "    eta = max(min(params[0], eta_range[1]), eta_range[0])\n",
    "    mu = max(min(params[1], mu_range[1]), mu_range[0])\n",
    "\n",
    "    # Limitar otros parámetros como antes\n",
    "    layers = int(max(min(params[2], max(layers_range)), min(layers_range)))\n",
    "    neurons_index = int(max(min(params[3], len(neurons_options) - 1), 0))\n",
    "    neurons = neurons_options[neurons_index]\n",
    "    offline = int(max(min(params[4], 1), 0))\n",
    "    error_function = int(max(min(params[5], 1), 0))\n",
    "\n",
    "    # Ejecutar el programa y obtener resultados\n",
    "    result = run_program(train_file, test_file, eta, mu, layers, neurons, offline, error_function)\n",
    "\n",
    "    if result:\n",
    "        return result[\"test_mean\"], result  # Devuelve el test_mean y el resto del resultado\n",
    "    return float(\"inf\"), None  # Penalizar si hay fallos y no hay resultado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(filename, results):\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Encabezado del CSV\n",
    "        writer.writerow([\"Eta\", \"Mu\", \"Layers\", \"Neurons\", \"Offline\", \"Error Function\",\n",
    "                         \"Train Mean\", \"Train Std\", \"Test Mean\", \"Test Std\"])\n",
    "        # Escribir cada fila de resultados\n",
    "        for row in results:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle principal actualizado\n",
    "def optimize_with_cma_es(dataset_key):\n",
    "    if dataset_key not in datasets:\n",
    "        print(f\"Dataset '{dataset_key}' no encontrado.\")\n",
    "        return\n",
    "\n",
    "    # Archivos de entrenamiento y prueba\n",
    "    train_file = datasets[dataset_key][\"train\"]\n",
    "    test_file = datasets[dataset_key][\"test\"]\n",
    "\n",
    "    # Configuración inicial de CMA-ES\n",
    "    initial_params = [0.01, 0.5, 2, 3, 1, 0]  # [eta, mu, layers, neurons_index, offline, error_function]\n",
    "\n",
    "    sigma = 0.1 # Rango de búsqueda inicial\n",
    "    es = cma.CMAEvolutionStrategy(\n",
    "    initial_params,\n",
    "    sigma,\n",
    "    {\n",
    "        'bounds': [\n",
    "            [eta_range[0], mu_range[0], 1, 0, 0, 0],  # Límite inferior\n",
    "            [eta_range[1], mu_range[1], max(layers_range), len(neurons_options) - 1, 1, 1]  # Límite superior\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "    # Resultados almacenados\n",
    "    results = []\n",
    "\n",
    "    # Bucle de optimización\n",
    "    while not es.stop():\n",
    "        solutions = es.ask()\n",
    "        fitness = []\n",
    "        for solution in solutions:\n",
    "            # Evaluar cada solución\n",
    "            test_mean, result = evaluate_fitness(solution, train_file, test_file)\n",
    "            fitness.append(test_mean)  # Usar solo test_mean como fitness\n",
    "\n",
    "            if result:  # Guardar resultados solo si son válidos\n",
    "                eta, mu = solution[0], solution[1]\n",
    "                layers = int(max(min(solution[2], max(layers_range)), min(layers_range)))\n",
    "                neurons_index = int(max(min(solution[3], len(neurons_options) - 1), 0))\n",
    "                neurons = neurons_options[neurons_index]\n",
    "                offline = int(max(min(solution[4], 1), 0))\n",
    "                error_function = int(max(min(solution[5], 1), 0))\n",
    "\n",
    "                # Almacenar resultados\n",
    "                results.append([\n",
    "                    eta, mu, layers, neurons, offline, error_function,\n",
    "                    result[\"train_mean\"], result[\"train_std\"], result[\"test_mean\"], result[\"test_std\"]\n",
    "                ])\n",
    "\n",
    "        # Informar a CMA-ES\n",
    "        es.tell(solutions, fitness)\n",
    "        print(f\"Generación {es.result.iterations}: Mejor error de prueba = {min(fitness)}\")\n",
    "\n",
    "\n",
    "    # Guardar resultados en CSV\n",
    "    csv_filename = f\"results_{dataset_key}.csv\"\n",
    "    save_to_csv(csv_filename, results)\n",
    "    print(f\"Resultados guardados en {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4_w,9)-aCMA-ES (mu_w=2.8,w_1=49%) in dimension 6 (seed=999069, Sun Dec 22 20:42:19 2024)\n",
      "Generación 1: Mejor error de prueba = 0.00012217\n",
      "Generación 2: Mejor error de prueba = 9.9249e-05\n",
      "Generación 3: Mejor error de prueba = 0.000100223\n",
      "Generación 4: Mejor error de prueba = 7.66904e-05\n",
      "Generación 5: Mejor error de prueba = 6.56835e-05\n",
      "Generación 6: Mejor error de prueba = 7.08655e-05\n",
      "Generación 7: Mejor error de prueba = 7.79664e-05\n",
      "Generación 8: Mejor error de prueba = 7.22491e-05\n",
      "Generación 9: Mejor error de prueba = 6.25398e-05\n",
      "Generación 10: Mejor error de prueba = 6.12768e-05\n",
      "Generación 11: Mejor error de prueba = 6.09224e-05\n",
      "Generación 12: Mejor error de prueba = 6.12289e-05\n",
      "Generación 13: Mejor error de prueba = 6.23809e-05\n",
      "Generación 14: Mejor error de prueba = 6.05429e-05\n",
      "Generación 15: Mejor error de prueba = 6.07448e-05\n",
      "Generación 16: Mejor error de prueba = 6.03083e-05\n",
      "Generación 17: Mejor error de prueba = 6.06412e-05\n",
      "Generación 18: Mejor error de prueba = 6.03532e-05\n",
      "Generación 19: Mejor error de prueba = 6.05147e-05\n",
      "Generación 20: Mejor error de prueba = 6.04381e-05\n",
      "Generación 21: Mejor error de prueba = 6.05204e-05\n",
      "Generación 22: Mejor error de prueba = 6.0573e-05\n",
      "Generación 23: Mejor error de prueba = 6.0473e-05\n",
      "Generación 24: Mejor error de prueba = 6.05165e-05\n",
      "Generación 25: Mejor error de prueba = 6.04936e-05\n",
      "Generación 26: Mejor error de prueba = 6.05613e-05\n",
      "Generación 27: Mejor error de prueba = 6.0502e-05\n",
      "Generación 28: Mejor error de prueba = 6.0466e-05\n",
      "Generación 29: Mejor error de prueba = 6.04895e-05\n",
      "Generación 30: Mejor error de prueba = 6.04132e-05\n",
      "Generación 31: Mejor error de prueba = 6.05108e-05\n",
      "Generación 32: Mejor error de prueba = 6.04809e-05\n",
      "Generación 33: Mejor error de prueba = 6.03497e-05\n",
      "Generación 34: Mejor error de prueba = 6.0499e-05\n",
      "Generación 35: Mejor error de prueba = 6.04741e-05\n",
      "Generación 36: Mejor error de prueba = 6.05044e-05\n",
      "Generación 37: Mejor error de prueba = 6.01629e-05\n",
      "Generación 38: Mejor error de prueba = 6.04495e-05\n",
      "Generación 39: Mejor error de prueba = 6.04907e-05\n",
      "Generación 40: Mejor error de prueba = 6.03884e-05\n",
      "Generación 41: Mejor error de prueba = 6.05538e-05\n",
      "Generación 42: Mejor error de prueba = 6.0314e-05\n",
      "Generación 43: Mejor error de prueba = 6.04742e-05\n",
      "Generación 44: Mejor error de prueba = 6.04952e-05\n",
      "Generación 45: Mejor error de prueba = 6.05005e-05\n",
      "Generación 46: Mejor error de prueba = 6.04199e-05\n",
      "Generación 47: Mejor error de prueba = 6.03631e-05\n",
      "Generación 48: Mejor error de prueba = 6.02822e-05\n",
      "Generación 49: Mejor error de prueba = 6.03329e-05\n",
      "Generación 50: Mejor error de prueba = 6.02488e-05\n",
      "Generación 51: Mejor error de prueba = 6.05306e-05\n",
      "Generación 52: Mejor error de prueba = 6.05027e-05\n",
      "Generación 53: Mejor error de prueba = 6.04854e-05\n",
      "Generación 54: Mejor error de prueba = 6.03177e-05\n",
      "Generación 55: Mejor error de prueba = 6.0448e-05\n",
      "Generación 56: Mejor error de prueba = 6.04326e-05\n",
      "Generación 57: Mejor error de prueba = 6.03916e-05\n",
      "Generación 58: Mejor error de prueba = 6.04831e-05\n",
      "Generación 59: Mejor error de prueba = 6.04754e-05\n",
      "Generación 60: Mejor error de prueba = 6.04803e-05\n",
      "Generación 61: Mejor error de prueba = 6.04767e-05\n",
      "Generación 62: Mejor error de prueba = 6.05009e-05\n",
      "Generación 63: Mejor error de prueba = 6.04482e-05\n",
      "Generación 64: Mejor error de prueba = 6.03329e-05\n",
      "Generación 65: Mejor error de prueba = 6.03869e-05\n",
      "Generación 66: Mejor error de prueba = 5.9918e-05\n",
      "Generación 67: Mejor error de prueba = 6.02372e-05\n",
      "Generación 68: Mejor error de prueba = 6.01419e-05\n",
      "Generación 69: Mejor error de prueba = 5.99121e-05\n",
      "Generación 70: Mejor error de prueba = 6.03742e-05\n",
      "Generación 71: Mejor error de prueba = 5.99723e-05\n",
      "Generación 72: Mejor error de prueba = 6.01044e-05\n",
      "Generación 73: Mejor error de prueba = 6.0274e-05\n",
      "Generación 74: Mejor error de prueba = 6.01595e-05\n",
      "Generación 75: Mejor error de prueba = 6.01587e-05\n",
      "Generación 76: Mejor error de prueba = 6.03349e-05\n",
      "Generación 77: Mejor error de prueba = 6.01607e-05\n",
      "Generación 78: Mejor error de prueba = 5.98962e-05\n",
      "Generación 79: Mejor error de prueba = 5.98819e-05\n",
      "Generación 80: Mejor error de prueba = 5.98839e-05\n",
      "Generación 81: Mejor error de prueba = 5.9918e-05\n",
      "Generación 82: Mejor error de prueba = 5.9995e-05\n",
      "Generación 83: Mejor error de prueba = 5.98849e-05\n",
      "Generación 84: Mejor error de prueba = 5.98822e-05\n",
      "Generación 85: Mejor error de prueba = 5.99459e-05\n",
      "Generación 86: Mejor error de prueba = 5.99802e-05\n",
      "Generación 87: Mejor error de prueba = 6.00528e-05\n",
      "Generación 88: Mejor error de prueba = 5.98859e-05\n",
      "Generación 89: Mejor error de prueba = 5.98928e-05\n",
      "Generación 90: Mejor error de prueba = 5.98831e-05\n",
      "Generación 91: Mejor error de prueba = 5.98854e-05\n",
      "Generación 92: Mejor error de prueba = 5.9883e-05\n",
      "Generación 93: Mejor error de prueba = 5.98859e-05\n",
      "Generación 94: Mejor error de prueba = 5.98858e-05\n",
      "Generación 95: Mejor error de prueba = 5.98865e-05\n",
      "Generación 96: Mejor error de prueba = 5.98818e-05\n",
      "Generación 97: Mejor error de prueba = 5.98816e-05\n",
      "Generación 98: Mejor error de prueba = 5.98822e-05\n",
      "Generación 99: Mejor error de prueba = 5.98817e-05\n",
      "Generación 100: Mejor error de prueba = 5.98831e-05\n",
      "Generación 101: Mejor error de prueba = 5.98821e-05\n",
      "Generación 102: Mejor error de prueba = 5.98822e-05\n",
      "Generación 103: Mejor error de prueba = 5.98822e-05\n",
      "Generación 104: Mejor error de prueba = 5.98817e-05\n",
      "Generación 105: Mejor error de prueba = 5.98817e-05\n",
      "Generación 106: Mejor error de prueba = 5.98818e-05\n",
      "Generación 107: Mejor error de prueba = 5.98819e-05\n",
      "Generación 108: Mejor error de prueba = 5.98822e-05\n",
      "Generación 109: Mejor error de prueba = 5.98816e-05\n",
      "Generación 110: Mejor error de prueba = 5.98816e-05\n",
      "Generación 111: Mejor error de prueba = 5.98816e-05\n",
      "Generación 112: Mejor error de prueba = 5.98816e-05\n",
      "Generación 113: Mejor error de prueba = 5.98816e-05\n",
      "Generación 114: Mejor error de prueba = 5.98816e-05\n",
      "Generación 115: Mejor error de prueba = 5.98816e-05\n",
      "Generación 116: Mejor error de prueba = 5.98816e-05\n",
      "Generación 117: Mejor error de prueba = 5.98816e-05\n",
      "Generación 118: Mejor error de prueba = 5.98816e-05\n",
      "Generación 119: Mejor error de prueba = 5.98816e-05\n",
      "Generación 120: Mejor error de prueba = 5.98815e-05\n",
      "Generación 121: Mejor error de prueba = 5.98816e-05\n",
      "Generación 122: Mejor error de prueba = 5.98816e-05\n",
      "Generación 123: Mejor error de prueba = 5.98816e-05\n",
      "Generación 124: Mejor error de prueba = 5.98816e-05\n",
      "Generación 125: Mejor error de prueba = 5.98815e-05\n",
      "Generación 126: Mejor error de prueba = 5.98816e-05\n",
      "Generación 127: Mejor error de prueba = 5.98816e-05\n",
      "Generación 128: Mejor error de prueba = 5.98815e-05\n",
      "Generación 129: Mejor error de prueba = 5.98815e-05\n",
      "Generación 130: Mejor error de prueba = 5.98815e-05\n",
      "Generación 131: Mejor error de prueba = 5.98815e-05\n",
      "Generación 132: Mejor error de prueba = 5.98816e-05\n",
      "Generación 133: Mejor error de prueba = 5.98816e-05\n",
      "Resultados guardados en results_xor.csv\n"
     ]
    }
   ],
   "source": [
    "nombre = \"xor\"\n",
    "optimize_with_cma_es(nombre)\n",
    "resultados.append(pd.read_csv(f\"results_{nombre}.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Eta        Mu  Layers  Neurons  Offline  Error Function  \\\n",
      "1072  0.700000  0.987025       2        8        0               0   \n",
      "1123  0.700000  0.987026       2        8        0               0   \n",
      "1144  0.700000  0.987026       2        8        0               0   \n",
      "1157  0.700000  0.987025       2        8        0               0   \n",
      "1167  0.700000  0.987026       2        8        0               0   \n",
      "1173  0.700000  0.987025       2        8        0               0   \n",
      "1178  0.700000  0.987025       2        8        0               0   \n",
      "872   0.699999  0.987021       2        8        0               0   \n",
      "972   0.700000  0.987018       2        8        0               0   \n",
      "975   0.699999  0.987027       2        8        0               0   \n",
      "\n",
      "      Train Mean  Train Std  Test Mean  Test Std  \n",
      "1072     0.00006    0.00002    0.00006   0.00002  \n",
      "1123     0.00006    0.00002    0.00006   0.00002  \n",
      "1144     0.00006    0.00002    0.00006   0.00002  \n",
      "1157     0.00006    0.00002    0.00006   0.00002  \n",
      "1167     0.00006    0.00002    0.00006   0.00002  \n",
      "1173     0.00006    0.00002    0.00006   0.00002  \n",
      "1178     0.00006    0.00002    0.00006   0.00002  \n",
      "872      0.00006    0.00002    0.00006   0.00002  \n",
      "972      0.00006    0.00002    0.00006   0.00002  \n",
      "975      0.00006    0.00002    0.00006   0.00002  \n"
     ]
    }
   ],
   "source": [
    "print(resultados[0].sort_values(by=[\"Test Mean\", \"Train Mean\", \"Layers\", \"Neurons\"], ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MULTI-OBJETIVE OPTIMIZATION: FOR THE CHEAPEST SET OF PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar un conjunto de parámetros\n",
    "def evaluate(params):\n",
    "    eta = np.clip(params[0], *eta_range)\n",
    "    mu = np.clip(params[1], *mu_range)\n",
    "    layers = layers_range[int(np.clip(params[2], 0, len(layers_range) - 1))]\n",
    "    neurons = neurons_options[int(np.clip(params[3], 0, len(neurons_options) - 1))]\n",
    "    offline = int(np.clip(params[4], 0, 1))\n",
    "    error_function = int(np.clip(params[5], 0, 1))\n",
    "\n",
    "    # Simular evaluación (reemplaza esto con tu evaluación real)\n",
    "    # Por ejemplo, puedes llamar a run_program aquí\n",
    "    train_error = np.random.rand()  # Simulación del error de entrenamiento\n",
    "    test_error = np.random.rand()   # Simulación del error de prueba\n",
    "    total_neurons = layers * neurons\n",
    "\n",
    "    return train_error, test_error, total_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.core.problem import Problem\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetworkOptimization(Problem):\n",
    "    def __init__(self):\n",
    "        super().__init__(n_var=6, n_obj=3, n_constr=0, xl=np.array([0.0001, 0.0001, 0, 0, 0, 0]), xu=np.array([0.7, 1.0, 3, 6, 1, 1]))\n",
    "\n",
    "    def _evaluate(self, X, out, *args, **kwargs):\n",
    "        # Aquí simulas la evaluación; reemplaza con tu lógica\n",
    "        results = []\n",
    "        for params in X:\n",
    "            train_error = np.random.rand()  # Simulación\n",
    "            test_error = np.random.rand()   # Simulación\n",
    "            total_neurons = params[2] * params[3]\n",
    "            results.append([train_error, test_error, total_neurons])\n",
    "        out[\"F\"] = np.array(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "n_gen |  n_eval |  n_nds  |     eps      |  indicator  \n",
      "=======================================================\n",
      "    1 |      50 |       8 |            - |            -\n",
      "    2 |     100 |      13 |  0.053944392 |        ideal\n",
      "    3 |     150 |      21 |  0.008735779 |        ideal\n",
      "    4 |     200 |      23 |  0.005039377 |        ideal\n",
      "    5 |     250 |      20 |  0.463356646 |        nadir\n",
      "    6 |     300 |      16 |  0.008766149 |        ideal\n",
      "    7 |     350 |      13 |  0.302628519 |        nadir\n",
      "    8 |     400 |      15 |  0.575434231 |        nadir\n",
      "    9 |     450 |      10 |  1.009981043 |        nadir\n",
      "   10 |     500 |      16 |  0.501005501 |        nadir\n",
      "   11 |     550 |      17 |  0.038251668 |        nadir\n",
      "   12 |     600 |      17 |  0.032338956 |        nadir\n",
      "   13 |     650 |      10 |  0.869731690 |        nadir\n",
      "   14 |     700 |      13 |  0.493177657 |        nadir\n",
      "   15 |     750 |      12 |  0.973077970 |        nadir\n",
      "   16 |     800 |      17 |  0.464822689 |        nadir\n",
      "   17 |     850 |      19 |  0.242706426 |        nadir\n",
      "   18 |     900 |      19 |  0.503605005 |        nadir\n",
      "   19 |     950 |      20 |  1.24576E+02 |        nadir\n",
      "   20 |    1000 |      21 |  0.003177889 |            f\n",
      "   21 |    1050 |      22 |  0.004290405 |            f\n",
      "   22 |    1100 |      26 |  0.711498484 |        nadir\n",
      "   23 |    1150 |      21 |  0.164633138 |        nadir\n",
      "   24 |    1200 |      22 |  0.004207197 |            f\n",
      "   25 |    1250 |      23 |  0.008134183 |            f\n",
      "   26 |    1300 |      23 |  0.587340734 |        nadir\n",
      "   27 |    1350 |      25 |  0.185989352 |        nadir\n",
      "   28 |    1400 |      29 |  0.222037362 |        nadir\n",
      "   29 |    1450 |      29 |  0.285408774 |        nadir\n",
      "   30 |    1500 |      31 |  0.017722292 |            f\n",
      "   31 |    1550 |      32 |  0.012556482 |            f\n",
      "   32 |    1600 |      30 |  0.262160636 |        nadir\n",
      "   33 |    1650 |      31 |  0.006016001 |            f\n",
      "   34 |    1700 |      34 |  0.122248988 |        nadir\n",
      "   35 |    1750 |      34 |  9.67901E+01 |        nadir\n",
      "   36 |    1800 |      30 |  0.052809283 |        nadir\n",
      "   37 |    1850 |      32 |  0.007868289 |            f\n",
      "   38 |    1900 |      32 |  0.138610699 |        nadir\n",
      "   39 |    1950 |      25 |  0.999705830 |        nadir\n",
      "   40 |    2000 |      25 |  0.294542098 |        nadir\n",
      "   41 |    2050 |      27 |  0.054137644 |        nadir\n",
      "   42 |    2100 |      31 |  0.016616789 |            f\n",
      "   43 |    2150 |      35 |  0.014136448 |        nadir\n",
      "   44 |    2200 |      34 |  0.012234618 |            f\n",
      "   45 |    2250 |      28 |  0.085363963 |        nadir\n",
      "   46 |    2300 |      32 |  0.029696619 |            f\n",
      "   47 |    2350 |      28 |  0.180050354 |        nadir\n",
      "   48 |    2400 |      27 |  0.135490822 |        nadir\n",
      "   49 |    2450 |      27 |  0.183481669 |        nadir\n",
      "   50 |    2500 |      29 |  0.297419735 |        nadir\n",
      "   51 |    2550 |      33 |  0.239361303 |        nadir\n",
      "   52 |    2600 |      35 |  0.168982741 |        nadir\n",
      "   53 |    2650 |      37 |  0.012859496 |            f\n",
      "   54 |    2700 |      40 |  0.007880842 |            f\n",
      "   55 |    2750 |      44 |  0.100159092 |        nadir\n",
      "   56 |    2800 |      42 |  0.049413449 |        nadir\n",
      "   57 |    2850 |      40 |  0.007479383 |            f\n",
      "   58 |    2900 |      37 |  0.036323072 |        nadir\n",
      "   59 |    2950 |      36 |  0.004969724 |            f\n",
      "   60 |    3000 |      40 |  0.010701674 |            f\n",
      "   61 |    3050 |      36 |  0.000585898 |            f\n",
      "   62 |    3100 |      36 |  0.000489603 |            f\n",
      "   63 |    3150 |      38 |  0.001292309 |            f\n",
      "   64 |    3200 |      38 |  0.088634073 |        nadir\n",
      "   65 |    3250 |      38 |  0.787441737 |        nadir\n",
      "   66 |    3300 |      35 |  0.073892176 |        nadir\n",
      "   67 |    3350 |      35 |  0.00000E+00 |            f\n",
      "   68 |    3400 |      33 |  0.002614403 |            f\n",
      "   69 |    3450 |      33 |  0.00000E+00 |            f\n",
      "   70 |    3500 |      34 |  0.000217754 |            f\n",
      "   71 |    3550 |      35 |  0.000951672 |            f\n",
      "   72 |    3600 |      35 |  0.00000E+00 |            f\n",
      "   73 |    3650 |      35 |  0.00000E+00 |            f\n",
      "   74 |    3700 |      35 |  0.00000E+00 |            f\n",
      "   75 |    3750 |      35 |  0.00000E+00 |            f\n",
      "   76 |    3800 |      35 |  0.00000E+00 |            f\n",
      "   77 |    3850 |      35 |  0.00000E+00 |            f\n",
      "   78 |    3900 |      35 |  0.00000E+00 |            f\n",
      "   79 |    3950 |      35 |  0.00000E+00 |            f\n",
      "   80 |    4000 |      35 |  0.00000E+00 |            f\n",
      "   81 |    4050 |      35 |  0.00000E+00 |            f\n",
      "   82 |    4100 |      35 |  0.00000E+00 |            f\n",
      "   83 |    4150 |      35 |  0.00000E+00 |            f\n",
      "   84 |    4200 |      35 |  0.00000E+00 |            f\n",
      "   85 |    4250 |      36 |  0.003743346 |            f\n",
      "   86 |    4300 |      36 |  0.00000E+00 |            f\n",
      "   87 |    4350 |      38 |  0.333158276 |        nadir\n",
      "   88 |    4400 |      38 |  0.268752808 |        nadir\n",
      "   89 |    4450 |      38 |  0.00000E+00 |            f\n",
      "   90 |    4500 |      39 |  0.000178554 |            f\n",
      "   91 |    4550 |      30 |  0.000600983 |            f\n",
      "   92 |    4600 |      30 |  0.00000E+00 |            f\n",
      "   93 |    4650 |      31 |  0.122490969 |        nadir\n",
      "   94 |    4700 |      31 |  0.00000E+00 |            f\n",
      "   95 |    4750 |      31 |  0.00000E+00 |            f\n",
      "   96 |    4800 |      31 |  0.00000E+00 |            f\n",
      "   97 |    4850 |      31 |  0.00000E+00 |            f\n",
      "   98 |    4900 |      31 |  0.00000E+00 |            f\n",
      "   99 |    4950 |      31 |  0.00000E+00 |            f\n",
      "  100 |    5000 |      32 |  0.001223364 |            f\n",
      "Mejores soluciones (frente de Pareto):\n",
      "Solución 1: [0.08781799 0.26936612 0.00884639 0.0369151  0.62911136 0.77750704], Objetivos: [0.21994429 0.00041394 0.00032657]\n",
      "Solución 2: [6.15322453e-01 3.63266213e-01 3.22491095e-04 1.83421107e-02\n",
      " 6.04882869e-01 7.84189358e-01], Objetivos: [4.48473588e-02 9.09645213e-03 5.91516737e-06]\n",
      "Solución 3: [0.60474353 0.20089176 0.00392696 0.00079431 0.15545876 0.70801047], Objetivos: [7.05101805e-03 1.30681483e-02 3.11922997e-06]\n",
      "Solución 4: [4.81165122e-01 6.73977093e-01 1.86553259e-06 1.13990364e-04\n",
      " 3.35421993e-01 7.81986144e-01], Objetivos: [9.08793386e-02 1.53368798e-02 2.12652739e-10]\n",
      "Solución 5: [0.08368149 0.52785163 0.2124381  0.00215105 0.36007196 0.71587717], Objetivos: [0.01239969 0.00333664 0.00045696]\n",
      "Solución 6: [5.26266618e-02 3.31835084e-01 2.18356521e-06 4.85998073e-03\n",
      " 4.04566431e-01 9.26992821e-02], Objetivos: [3.40542730e-01 3.75999467e-03 1.06120849e-08]\n",
      "Solución 7: [0.09550626 0.07837627 1.56843139 3.28552966 0.35450084 0.16314833], Objetivos: [3.02639464e-04 5.18225813e-01 5.15312786e+00]\n",
      "Solución 8: [0.09526976 0.17556785 1.81187704 0.07902189 0.37328449 0.18238115], Objetivos: [4.11077544e-04 8.14184453e-01 1.43177956e-01]\n",
      "Solución 9: [4.84454199e-01 3.83348647e-01 8.33956386e-02 1.72828696e-07\n",
      " 3.08691667e-01 1.77119468e-01], Objetivos: [2.44743952e-01 3.60571649e-03 1.44131595e-08]\n",
      "Solución 10: [4.67103452e-01 3.71915586e-01 7.55559230e-08 3.13983065e-09\n",
      " 3.77624288e-01 2.39118441e-01], Objetivos: [4.99482539e-01 7.76697738e-04 2.37232803e-16]\n",
      "Solución 11: [4.81165122e-01 6.72388522e-01 6.08465211e-08 1.73686367e-09\n",
      " 3.36846956e-01 2.38318303e-01], Objetivos: [5.70139193e-02 1.62016997e-02 1.05682112e-16]\n",
      "Solución 12: [2.22611428e-01 3.74021725e-01 7.57292955e-08 2.46037367e-07\n",
      " 3.77624288e-01 1.18186570e-01], Objetivos: [2.88974896e-01 8.41147280e-03 1.86322364e-14]\n",
      "Solución 13: [9.20033556e-02 2.53145244e-01 7.99306017e-07 2.46037367e-07\n",
      " 6.24075894e-01 7.94829756e-01], Objetivos: [1.44361634e-01 8.54189366e-03 1.96659148e-13]\n",
      "Solución 14: [1.06038647e-01 8.32351225e-01 9.47423330e-08 1.74970713e-11\n",
      " 6.99709840e-01 1.76472234e-01], Objetivos: [1.59458061e-01 9.41238008e-03 1.65771336e-18]\n",
      "Solución 15: [1.18249614e-01 7.37199978e-01 8.01091099e-11 2.40000500e-07\n",
      " 3.76086209e-01 1.67575678e-01], Objetivos: [1.27089735e-01 1.64543445e-02 1.92262264e-17]\n",
      "Solución 16: [1.12413558e-01 7.07961650e-01 3.19545144e-11 2.92663077e-10\n",
      " 6.60544319e-01 8.32672097e-01], Objetivos: [3.25662388e-02 7.52950821e-02 9.35190651e-21]\n",
      "Solución 17: [4.02898643e-01 3.31990299e-01 9.78768381e-10 3.01776460e-03\n",
      " 3.12579338e-01 2.08177664e-02], Objetivos: [1.55836973e-01 8.47605863e-03 2.95369257e-12]\n",
      "Solución 18: [1.29131078e-01 7.50105038e-01 3.71724891e-11 2.50770300e-09\n",
      " 3.16687479e-01 1.69608605e-01], Objetivos: [5.35610070e-01 9.98897522e-03 9.32175624e-20]\n",
      "Solución 19: [4.87238535e-01 4.20479566e-01 3.76541364e-12 2.54080560e-09\n",
      " 9.24106579e-01 2.52559174e-01], Objetivos: [1.71670122e-02 6.44364005e-02 9.56718407e-21]\n",
      "Solución 20: [1.25182596e-01 8.98063899e-01 9.47423330e-08 2.85362359e-12\n",
      " 9.24458426e-01 2.51522039e-01], Objetivos: [6.70646056e-04 7.24214079e-01 2.70358956e-19]\n",
      "Solución 21: [4.36237394e-01 7.14820301e-01 8.49287259e-08 3.21498105e-09\n",
      " 3.77624288e-01 2.39118441e-01], Objetivos: [1.10014285e-01 9.13445108e-03 2.73044244e-16]\n",
      "Solución 22: [0.36522109 0.74342201 0.00376347 0.         0.69787878 0.82835273], Objetivos: [0.14515916 0.02473576 0.        ]\n",
      "Solución 23: [0.433239   0.08104164 1.61419485 0.         0.69988661 0.16329388], Objetivos: [0.00550285 0.26573169 0.        ]\n",
      "Solución 24: [5.13163622e-01 7.83843954e-01 4.04466038e-11 0.00000000e+00\n",
      " 8.65644958e-01 2.06974946e-01], Objetivos: [0.15167132 0.01068642 0.        ]\n",
      "Solución 25: [4.68333581e-01 3.71915586e-01 5.02994478e-08 3.08368325e-09\n",
      " 7.32522200e-01 3.78262213e-01], Objetivos: [1.17427230e-01 9.11271608e-03 1.55107564e-16]\n",
      "Solución 26: [0.0677273  0.76970273 1.81723685 0.         0.72229542 0.05102148], Objetivos: [0.03170717 0.11752882 0.        ]\n",
      "Solución 27: [1.09776359e-01 8.35677257e-01 9.17861895e-08 1.19804456e-04\n",
      " 3.98638246e-01 3.24311660e-01], Objetivos: [2.48443131e-01 5.76802512e-03 1.09963945e-11]\n",
      "Solución 28: [4.99674762e-02 3.31835084e-01 2.18356521e-06 2.19620946e-02\n",
      " 4.03272940e-01 1.61436805e-01], Objetivos: [9.93991953e-02 1.26334561e-02 4.79556658e-08]\n",
      "Solución 29: [2.16018070e-01 7.18294515e-01 2.56597274e-08 2.85362359e-12\n",
      " 3.26322403e-01 1.67270605e-01], Objetivos: [7.81203703e-04 2.53251693e-02 7.32232032e-20]\n",
      "Solución 30: [0.433239   0.05162963 1.61419485 0.         0.69988661 0.16329388], Objetivos: [0.09085787 0.05930554 0.        ]\n",
      "Solución 31: [1.31117114e-01 7.69058533e-01 3.71724891e-11 2.50770300e-09\n",
      " 3.82874733e-01 1.88419526e-01], Objetivos: [5.63967075e-01 5.67389256e-04 9.32175624e-20]\n",
      "Solución 32: [1.29131078e-01 6.86288624e-01 1.04616139e-08 2.50770300e-09\n",
      " 7.53333747e-01 1.63083959e-01], Objetivos: [4.77622185e-01 5.11727823e-03 2.62346205e-17]\n"
     ]
    }
   ],
   "source": [
    "# Crear el problema\n",
    "nn_problem = NeuralNetworkOptimization()\n",
    "algorithm = NSGA2(pop_size=50)\n",
    "termination = get_termination(\"n_gen\", 100)\n",
    "\n",
    "res = minimize(nn_problem, algorithm, termination, verbose=True)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Mejores soluciones (frente de Pareto):\")\n",
    "for i, x in enumerate(res.X):\n",
    "    print(f\"Solución {i+1}: {x}, Objetivos: {res.F[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "n_gen |  n_eval |     igd      |      gd      |      hv     \n",
      "============================================================\n",
      "    1 |      50 |  2.252241564 |  3.031767728 |  0.00000E+00\n",
      "    2 |     100 |  2.252241564 |  2.494130531 |  0.00000E+00\n",
      "    3 |     150 |  2.215812395 |  2.572530467 |  0.00000E+00\n",
      "    4 |     200 |  2.016426153 |  2.482033768 |  0.00000E+00\n",
      "    5 |     250 |  1.988445931 |  2.549444967 |  0.00000E+00\n",
      "    6 |     300 |  1.922095162 |  2.508904603 |  0.00000E+00\n",
      "    7 |     350 |  1.862160884 |  2.285705783 |  0.00000E+00\n",
      "    8 |     400 |  1.651305250 |  2.164219691 |  0.00000E+00\n",
      "    9 |     450 |  1.417692983 |  2.063133885 |  0.00000E+00\n",
      "   10 |     500 |  1.417692983 |  2.037856312 |  0.00000E+00\n",
      "   11 |     550 |  1.389750335 |  1.810408114 |  0.00000E+00\n",
      "   12 |     600 |  1.318325016 |  1.757115952 |  0.00000E+00\n",
      "   13 |     650 |  1.268551607 |  1.698874449 |  0.00000E+00\n",
      "   14 |     700 |  1.171794212 |  1.553478245 |  0.00000E+00\n",
      "   15 |     750 |  1.099461965 |  1.331474111 |  0.00000E+00\n",
      "   16 |     800 |  0.965168449 |  1.327520081 |  0.00000E+00\n",
      "   17 |     850 |  0.931653692 |  1.178635940 |  0.00000E+00\n",
      "   18 |     900 |  0.871221369 |  1.038893313 |  0.00000E+00\n",
      "   19 |     950 |  0.816195215 |  1.040331533 |  0.000965089\n",
      "   20 |    1000 |  0.795572214 |  1.026687793 |  0.001032800\n",
      "   21 |    1050 |  0.678179005 |  0.912330818 |  0.022802729\n",
      "   22 |    1100 |  0.677697515 |  0.788971471 |  0.022878388\n",
      "   23 |    1150 |  0.637914290 |  0.761318421 |  0.034142142\n",
      "   24 |    1200 |  0.614087986 |  0.728820005 |  0.050060195\n",
      "   25 |    1250 |  0.600124550 |  0.739129567 |  0.054956360\n",
      "   26 |    1300 |  0.580567704 |  0.721392765 |  0.070810673\n",
      "   27 |    1350 |  0.556878005 |  0.678617527 |  0.074830837\n",
      "   28 |    1400 |  0.527152353 |  0.633351895 |  0.084541422\n",
      "   29 |    1450 |  0.515604058 |  0.618628402 |  0.087469139\n",
      "   30 |    1500 |  0.509752554 |  0.593445342 |  0.099434143\n",
      "   31 |    1550 |  0.500118231 |  0.586655044 |  0.106888999\n",
      "   32 |    1600 |  0.475316104 |  0.546810939 |  0.111005693\n",
      "   33 |    1650 |  0.468695282 |  0.523440003 |  0.114921578\n",
      "   34 |    1700 |  0.461563589 |  0.509307577 |  0.125964302\n",
      "   35 |    1750 |  0.454448522 |  0.519709530 |  0.132515092\n",
      "   36 |    1800 |  0.442980655 |  0.470748236 |  0.139449304\n",
      "   37 |    1850 |  0.436502390 |  0.473576679 |  0.150817969\n",
      "   38 |    1900 |  0.428761398 |  0.470077166 |  0.158435463\n",
      "   39 |    1950 |  0.411696969 |  0.447386835 |  0.166010818\n",
      "   40 |    2000 |  0.393304430 |  0.435274160 |  0.181730204\n",
      "   41 |    2050 |  0.371111889 |  0.410068742 |  0.196072678\n",
      "   42 |    2100 |  0.366141523 |  0.404814046 |  0.206819461\n",
      "   43 |    2150 |  0.352336187 |  0.396375636 |  0.213795555\n",
      "   44 |    2200 |  0.348579725 |  0.397934566 |  0.217663171\n",
      "   45 |    2250 |  0.339940884 |  0.383843251 |  0.228130963\n",
      "   46 |    2300 |  0.330933438 |  0.372255101 |  0.236270505\n",
      "   47 |    2350 |  0.317855097 |  0.369319966 |  0.245932364\n",
      "   48 |    2400 |  0.311625284 |  0.363063159 |  0.255474514\n",
      "   49 |    2450 |  0.301223142 |  0.332327613 |  0.263795844\n",
      "   50 |    2500 |  0.287497875 |  0.323359739 |  0.273758805\n",
      "   51 |    2550 |  0.276799200 |  0.313011912 |  0.288056513\n",
      "   52 |    2600 |  0.268817051 |  0.308166106 |  0.298284175\n",
      "   53 |    2650 |  0.256725464 |  0.313419089 |  0.307134351\n",
      "   54 |    2700 |  0.247029969 |  0.301645695 |  0.320193423\n",
      "   55 |    2750 |  0.243858432 |  0.281640274 |  0.327027747\n",
      "   56 |    2800 |  0.232887873 |  0.276777447 |  0.337397549\n",
      "   57 |    2850 |  0.221359446 |  0.270918042 |  0.350167837\n",
      "   58 |    2900 |  0.216436469 |  0.243746005 |  0.357038911\n",
      "   59 |    2950 |  0.206590721 |  0.239952556 |  0.370589852\n",
      "   60 |    3000 |  0.194794081 |  0.218121266 |  0.386759167\n",
      "   61 |    3050 |  0.189347084 |  0.211818411 |  0.396299533\n",
      "   62 |    3100 |  0.182995133 |  0.193375436 |  0.402527335\n",
      "   63 |    3150 |  0.172780396 |  0.187355063 |  0.415981464\n",
      "   64 |    3200 |  0.162892953 |  0.180430345 |  0.426903559\n",
      "   65 |    3250 |  0.158416155 |  0.169706460 |  0.434043958\n",
      "   66 |    3300 |  0.155615864 |  0.162474165 |  0.440131607\n",
      "   67 |    3350 |  0.153343174 |  0.158201441 |  0.446414702\n",
      "   68 |    3400 |  0.143373899 |  0.150370848 |  0.455181034\n",
      "   69 |    3450 |  0.138535654 |  0.149686387 |  0.460405879\n",
      "   70 |    3500 |  0.130946152 |  0.139720218 |  0.470099741\n",
      "   71 |    3550 |  0.126067757 |  0.131639818 |  0.477321187\n",
      "   72 |    3600 |  0.121509654 |  0.127084763 |  0.485015667\n",
      "   73 |    3650 |  0.114797408 |  0.121543313 |  0.492044683\n",
      "   74 |    3700 |  0.111851968 |  0.116502301 |  0.495445084\n",
      "   75 |    3750 |  0.107830734 |  0.110345612 |  0.502358185\n",
      "   76 |    3800 |  0.105905505 |  0.109319153 |  0.505003205\n",
      "   77 |    3850 |  0.102512606 |  0.105801979 |  0.509675294\n",
      "   78 |    3900 |  0.097958137 |  0.101337260 |  0.515744260\n",
      "   79 |    3950 |  0.093877274 |  0.095764447 |  0.523198477\n",
      "   80 |    4000 |  0.089709158 |  0.091282863 |  0.529450041\n",
      "   81 |    4050 |  0.087499354 |  0.090183900 |  0.532331012\n",
      "   82 |    4100 |  0.084416672 |  0.086586700 |  0.537532557\n",
      "   83 |    4150 |  0.081926447 |  0.085112692 |  0.541738370\n",
      "   84 |    4200 |  0.078436027 |  0.081879678 |  0.546776188\n",
      "   85 |    4250 |  0.076790840 |  0.080427452 |  0.549663713\n",
      "   86 |    4300 |  0.075237778 |  0.078217657 |  0.552281070\n",
      "   87 |    4350 |  0.074196464 |  0.076730551 |  0.553920898\n",
      "   88 |    4400 |  0.070509306 |  0.073555965 |  0.557550563\n",
      "   89 |    4450 |  0.068537312 |  0.072568947 |  0.560058755\n",
      "   90 |    4500 |  0.065532921 |  0.071054431 |  0.563892513\n",
      "   91 |    4550 |  0.064175361 |  0.067466875 |  0.565818432\n",
      "   92 |    4600 |  0.062503006 |  0.064354807 |  0.569423977\n",
      "   93 |    4650 |  0.060279352 |  0.064027979 |  0.572478688\n",
      "   94 |    4700 |  0.058766049 |  0.062031299 |  0.575011753\n",
      "   95 |    4750 |  0.057432854 |  0.058718164 |  0.577261505\n",
      "   96 |    4800 |  0.055211726 |  0.055824882 |  0.580067124\n",
      "   97 |    4850 |  0.054402109 |  0.054702416 |  0.581910391\n",
      "   98 |    4900 |  0.052814871 |  0.053645005 |  0.583905084\n",
      "   99 |    4950 |  0.051296749 |  0.051980902 |  0.586514758\n",
      "  100 |    5000 |  0.050903922 |  0.051669820 |  0.586984380\n",
      "Soluciones:\n",
      "[[9.99814769e-01 4.81577148e-02 1.90946502e-02 ... 2.66216703e-02\n",
      "  2.95065515e-03 2.40499936e-02]\n",
      " [1.01964356e-07 3.70851461e-02 3.97775852e-02 ... 2.18900159e-02\n",
      "  4.33465475e-03 3.41074615e-03]\n",
      " [8.74085072e-03 4.09440178e-02 7.53103495e-03 ... 1.90033809e-02\n",
      "  4.40899176e-03 3.66729458e-03]\n",
      " ...\n",
      " [6.87012768e-01 4.74711970e-02 2.92834911e-02 ... 2.13224830e-02\n",
      "  4.41012339e-03 2.71334516e-02]\n",
      " [1.13126449e-01 4.03610996e-02 1.20790431e-03 ... 2.18965310e-02\n",
      "  3.75130927e-03 2.35521508e-02]\n",
      " [9.87523846e-01 4.10314862e-02 2.86645030e-02 ... 2.20965477e-02\n",
      "  4.28704579e-03 2.89759227e-03]]\n",
      "Objetivos:\n",
      "[[9.99814769e-01 4.13134998e-02]\n",
      " [1.01964356e-07 1.11401272e+00]\n",
      " [8.74085072e-03 9.83237538e-01]\n",
      " [5.52680717e-04 1.06972081e+00]\n",
      " [5.53039437e-01 3.17532499e-01]\n",
      " [5.85687066e-01 2.91987369e-01]\n",
      " [2.48521003e-02 9.32271246e-01]\n",
      " [5.97825307e-02 8.28772281e-01]\n",
      " [7.96726590e-01 1.68600764e-01]\n",
      " [4.68994572e-01 3.69209491e-01]\n",
      " [7.03680001e-01 2.24988136e-01]\n",
      " [4.25652769e-02 8.76816505e-01]\n",
      " [7.62972909e-01 1.96189899e-01]\n",
      " [2.18108510e-01 5.97148311e-01]\n",
      " [1.49911784e-01 6.89205957e-01]\n",
      " [7.44580852e-01 2.07543900e-01]\n",
      " [2.01440714e-01 6.26038178e-01]\n",
      " [5.17514897e-01 3.36528156e-01]\n",
      " [9.40026626e-01 9.00307177e-02]\n",
      " [8.29231086e-01 1.59455878e-01]\n",
      " [5.01204287e-01 3.49284929e-01]\n",
      " [9.60725409e-01 6.69119021e-02]\n",
      " [2.51363965e-01 5.84829673e-01]\n",
      " [3.04437223e-01 5.37579085e-01]\n",
      " [3.41197819e-02 8.97156568e-01]\n",
      " [3.23095045e-01 5.06537196e-01]\n",
      " [9.17085420e-01 1.06936087e-01]\n",
      " [4.56072357e-01 4.08654655e-01]\n",
      " [6.32001391e-01 2.87413430e-01]\n",
      " [2.86253409e-01 5.41915856e-01]\n",
      " [8.95183378e-01 1.14754621e-01]\n",
      " [4.19066924e-01 4.35859364e-01]\n",
      " [1.73117228e-01 6.56872885e-01]\n",
      " [4.44998825e-01 4.14720053e-01]\n",
      " [6.59173431e-01 2.69832915e-01]\n",
      " [8.47652273e-01 1.38328652e-01]\n",
      " [7.66346857e-02 8.03539448e-01]\n",
      " [1.39029659e-01 7.13644341e-01]\n",
      " [3.31117286e-01 4.93230853e-01]\n",
      " [1.00216416e-01 7.72189457e-01]\n",
      " [4.10140311e-01 4.44573308e-01]\n",
      " [7.97475683e-02 7.88826889e-01]\n",
      " [3.79154781e-01 4.49956360e-01]\n",
      " [2.53791144e-01 5.53750638e-01]\n",
      " [8.74117393e-01 1.28796612e-01]\n",
      " [1.84515178e-01 6.51458002e-01]\n",
      " [3.72684412e-01 4.58527935e-01]\n",
      " [6.87012768e-01 2.43248558e-01]\n",
      " [1.13126449e-01 7.51786123e-01]\n",
      " [9.87523846e-01 5.89717414e-02]]\n"
     ]
    }
   ],
   "source": [
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.factory import get_problem, get_termination\n",
    "from pymoo.optimize import minimize\n",
    "\n",
    "# Crear un problema simple\n",
    "problem = get_problem(\"zdt1\")\n",
    "\n",
    "# Configurar NSGA-II\n",
    "algorithm = NSGA2(pop_size=50)\n",
    "\n",
    "# Configurar la terminación\n",
    "termination = get_termination(\"n_gen\", 100)\n",
    "\n",
    "\n",
    "# Ejecutar la optimización\n",
    "res = minimize(problem, algorithm, termination, verbose=True)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Soluciones:\")\n",
    "print(res.X)\n",
    "print(\"Objetivos:\")\n",
    "print(res.F)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
